{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pi(tf.keras.Model):\n",
    "    def __init__(self, action_dim = 1):\n",
    "        super(Pi, self).__init__()\n",
    "        self.dense_1 = tf.keras.layers.Dense(64, 'relu')\n",
    "        self.dense_2 = tf.keras.layers.Dense(64, 'relu')\n",
    "        self.dense_3 = tf.keras.layers.Dense(action_dim, 'softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense_1(inputs)\n",
    "        x = self.dense_2(x)\n",
    "        return self.dense_3(x)\n",
    "    \n",
    "    def process(self, observations):\n",
    "        # Process batch observations using `call(x)` behind-the-scenes\n",
    "        action_probabilities = self.predict_on_batch(observations)\n",
    "        return action_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, action_dim = 1):\n",
    "        self.policy_net = Pi(action_dim = action_dim)\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        self.onpolicy_reset()\n",
    "    \n",
    "    def onpolicy_reset(self):\n",
    "        self.rewards = []\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        \n",
    "    def policy(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        state = tf.convert_to_tensor(state, dtype = tf.float32)\n",
    "        action_logits = self.policy_net(state)\n",
    "        action = tf.random.categorical( tf.math.log(action_logits), num_samples = 1 )\n",
    "        return action\n",
    "    \n",
    "    def act(self, state):\n",
    "        action = self.policy(state).numpy()\n",
    "        return action.squeeze()\n",
    "    \n",
    "    def learn(self):\n",
    "        discounted_reward = 0\n",
    "        discounted_rewards = []\n",
    "        for r in self.rewards:\n",
    "            discounted_reward = r + self.gamma * discounted_reward\n",
    "            discounted_rewards.append(discounted_reward)\n",
    "        discounted_rewards.reverse()\n",
    "        \n",
    "        for state, reward, action in zip(self.states, discounted_rewards, self.actions):\n",
    "            with tf.GradientTape() as tape:\n",
    "                action_probabilities = self.policy_net(np.array([state]), training = True)\n",
    "                loss = self.loss(action_probabilities, action, reward)\n",
    "            grads = tape.gradient(loss, self.policy_net.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.policy_net.trainable_variables))\n",
    "                \n",
    "    def loss(self, action_probabilities, action, reward):\n",
    "        dist = tfp.distributions.Categorical(\n",
    "            probs = action_probabilities, dtype = tf.float32\n",
    "        )\n",
    "        log_prob = dist.log_prob(action)\n",
    "        loss = -log_prob * reward\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode#:0 ep_reward:19.0\n",
      "\n",
      "Episode#:1 ep_reward:16.0\n",
      "\n",
      "Episode#:2 ep_reward:18.0\n",
      "\n",
      "Episode#:3 ep_reward:28.0\n",
      "\n",
      "Episode#:4 ep_reward:14.0\n",
      "\n",
      "Episode#:5 ep_reward:26.0\n",
      "\n",
      "Episode#:6 ep_reward:17.0\n",
      "\n",
      "Episode#:7 ep_reward:20.0\n",
      "\n",
      "Episode#:8 ep_reward:11.0\n",
      "\n",
      "Episode#:9 ep_reward:11.0\n",
      "\n",
      "Episode#:10 ep_reward:17.0\n",
      "\n",
      "Episode#:11 ep_reward:9.0\n",
      "\n",
      "Episode#:12 ep_reward:37.0\n",
      "\n",
      "Episode#:13 ep_reward:33.0\n",
      "\n",
      "Episode#:14 ep_reward:19.0\n",
      "\n",
      "Episode#:15 ep_reward:34.0\n",
      "\n",
      "Episode#:16 ep_reward:39.0\n",
      "\n",
      "Episode#:17 ep_reward:38.0\n",
      "\n",
      "Episode#:18 ep_reward:30.0\n",
      "\n",
      "Episode#:19 ep_reward:15.0\n",
      "\n",
      "Episode#:20 ep_reward:17.0\n",
      "\n",
      "Episode#:21 ep_reward:19.0\n",
      "\n",
      "Episode#:22 ep_reward:22.0\n",
      "\n",
      "Episode#:23 ep_reward:14.0\n",
      "\n",
      "Episode#:24 ep_reward:15.0\n",
      "\n",
      "Episode#:25 ep_reward:10.0\n",
      "\n",
      "Episode#:26 ep_reward:26.0\n",
      "\n",
      "Episode#:27 ep_reward:42.0\n",
      "\n",
      "Episode#:28 ep_reward:25.0\n",
      "\n",
      "Episode#:29 ep_reward:63.0\n",
      "\n",
      "Episode#:30 ep_reward:55.0\n",
      "\n",
      "Episode#:31 ep_reward:48.0\n",
      "\n",
      "Episode#:32 ep_reward:38.0\n",
      "\n",
      "Episode#:33 ep_reward:14.0\n",
      "\n",
      "Episode#:34 ep_reward:19.0\n",
      "\n",
      "Episode#:35 ep_reward:15.0\n",
      "\n",
      "Episode#:36 ep_reward:50.0\n",
      "\n",
      "Episode#:37 ep_reward:52.0\n",
      "\n",
      "Episode#:38 ep_reward:132.0\n",
      "\n",
      "Episode#:39 ep_reward:30.0\n",
      "\n",
      "Episode#:40 ep_reward:41.0\n",
      "\n",
      "Episode#:41 ep_reward:25.0\n",
      "\n",
      "Episode#:42 ep_reward:28.0\n",
      "\n",
      "Episode#:43 ep_reward:34.0\n",
      "\n",
      "Episode#:44 ep_reward:56.0\n",
      "\n",
      "Episode#:45 ep_reward:98.0\n",
      "\n",
      "Episode#:46 ep_reward:28.0\n",
      "\n",
      "Episode#:47 ep_reward:154.0\n",
      "\n",
      "Episode#:48 ep_reward:107.0\n",
      "\n",
      "Episode#:49 ep_reward:21.0\n",
      "\n",
      "Episode#:50 ep_reward:112.0\n",
      "\n",
      "Episode#:51 ep_reward:59.0\n",
      "\n",
      "Episode#:52 ep_reward:187.0\n",
      "\n",
      "Episode#:53 ep_reward:81.0\n",
      "\n",
      "Episode#:54 ep_reward:56.0\n",
      "\n",
      "Episode#:55 ep_reward:22.0\n",
      "\n",
      "Episode#:56 ep_reward:60.0\n",
      "\n",
      "Episode#:57 ep_reward:49.0\n",
      "\n",
      "Episode#:58 ep_reward:37.0\n",
      "\n",
      "Episode#:59 ep_reward:51.0\n",
      "\n",
      "Episode#:60 ep_reward:18.0\n",
      "\n",
      "Episode#:61 ep_reward:109.0\n",
      "\n",
      "Episode#:62 ep_reward:128.0\n",
      "\n",
      "Episode#:63 ep_reward:65.0\n",
      "\n",
      "Episode#:64 ep_reward:132.0\n",
      "\n",
      "Episode#:65 ep_reward:80.0\n",
      "\n",
      "Episode#:66 ep_reward:96.0\n",
      "\n",
      "Episode#:67 ep_reward:72.0\n",
      "\n",
      "Episode#:68 ep_reward:53.0\n",
      "\n",
      "Episode#:69 ep_reward:115.0\n",
      "\n",
      "Episode#:70 ep_reward:89.0\n",
      "\n",
      "Episode#:71 ep_reward:99.0\n",
      "\n",
      "Episode#:72 ep_reward:130.0\n",
      "\n",
      "Episode#:73 ep_reward:148.0\n",
      "\n",
      "Episode#:74 ep_reward:128.0\n",
      "\n",
      "Episode#:75 ep_reward:166.0\n",
      "\n",
      "Episode#:76 ep_reward:130.0\n",
      "\n",
      "Episode#:77 ep_reward:41.0\n",
      "\n",
      "Episode#:78 ep_reward:103.0\n",
      "\n",
      "Episode#:79 ep_reward:152.0\n",
      "\n",
      "Episode#:80 ep_reward:166.0\n",
      "\n",
      "Episode#:81 ep_reward:199.0\n",
      "\n",
      "Episode#:82 ep_reward:162.0\n",
      "\n",
      "Episode#:83 ep_reward:181.0\n",
      "\n",
      "Episode#:84 ep_reward:129.0\n",
      "\n",
      "Episode#:85 ep_reward:174.0\n",
      "\n",
      "Episode#:86 ep_reward:128.0\n",
      "\n",
      "Episode#:87 ep_reward:166.0\n",
      "\n",
      "Episode#:88 ep_reward:137.0\n",
      "\n",
      "Episode#:89 ep_reward:138.0\n",
      "\n",
      "Episode#:90 ep_reward:160.0\n",
      "\n",
      "Episode#:91 ep_reward:53.0\n",
      "\n",
      "Episode#:92 ep_reward:32.0\n",
      "\n",
      "Episode#:93 ep_reward:79.0\n",
      "\n",
      "Episode#:94 ep_reward:77.0\n",
      "\n",
      "Episode#:95 ep_reward:199.0\n",
      "\n",
      "Episode#:96 ep_reward:84.0\n",
      "\n",
      "Episode#:97 ep_reward:199.0\n",
      "\n",
      "Episode#:98 ep_reward:199.0\n",
      "\n",
      "Episode#:99 ep_reward:140.0\n",
      "\n",
      "Episode#:100 ep_reward:38.0\n",
      "\n",
      "Episode#:101 ep_reward:89.0\n",
      "\n",
      "Episode#:102 ep_reward:94.0\n",
      "\n",
      "Episode#:103 ep_reward:199.0\n",
      "\n",
      "Episode#:104 ep_reward:199.0\n",
      "\n",
      "Episode#:105 ep_reward:199.0\n",
      "\n",
      "Episode#:106 ep_reward:199.0\n",
      "\n",
      "Episode#:107 ep_reward:199.0\n",
      "\n",
      "Episode#:108 ep_reward:199.0\n",
      "\n",
      "Episode#:109 ep_reward:199.0\n",
      "\n",
      "Episode#:110 ep_reward:178.0\n",
      "\n",
      "Episode#:111 ep_reward:199.0\n",
      "\n",
      "Episode#:112 ep_reward:72.0\n",
      "\n",
      "Episode#:113 ep_reward:198.0\n",
      "\n",
      "Episode#:114 ep_reward:178.0\n",
      "\n",
      "Episode#:115 ep_reward:42.0\n",
      "\n",
      "Episode#:116 ep_reward:87.0\n",
      "\n",
      "Episode#:117 ep_reward:71.0\n",
      "\n",
      "Episode#:118 ep_reward:130.0\n",
      "\n",
      "Episode#:119 ep_reward:53.0\n",
      "\n",
      "Episode#:120 ep_reward:114.0\n",
      "\n",
      "Episode#:121 ep_reward:89.0\n",
      "\n",
      "Episode#:122 ep_reward:148.0\n",
      "\n",
      "Episode#:123 ep_reward:59.0\n",
      "\n",
      "Episode#:124 ep_reward:59.0\n",
      "\n",
      "Episode#:125 ep_reward:42.0\n",
      "\n",
      "Episode#:126 ep_reward:23.0\n",
      "\n",
      "Episode#:127 ep_reward:26.0\n",
      "\n",
      "Episode#:128 ep_reward:73.0\n",
      "\n",
      "Episode#:129 ep_reward:145.0\n",
      "\n",
      "Episode#:130 ep_reward:66.0\n",
      "\n",
      "Episode#:131 ep_reward:20.0\n",
      "\n",
      "Episode#:132 ep_reward:19.0\n",
      "\n",
      "Episode#:133 ep_reward:15.0\n",
      "\n",
      "Episode#:134 ep_reward:130.0\n",
      "\n",
      "Episode#:135 ep_reward:176.0\n",
      "\n",
      "Episode#:136 ep_reward:142.0\n",
      "\n",
      "Episode#:137 ep_reward:93.0\n",
      "\n",
      "Episode#:138 ep_reward:104.0\n",
      "\n",
      "Episode#:139 ep_reward:81.0\n",
      "\n",
      "Episode#:140 ep_reward:119.0\n",
      "\n",
      "Episode#:141 ep_reward:140.0\n",
      "\n",
      "Episode#:142 ep_reward:154.0\n",
      "\n",
      "Episode#:143 ep_reward:160.0\n",
      "\n",
      "Episode#:144 ep_reward:148.0\n",
      "\n",
      "Episode#:145 ep_reward:170.0\n",
      "\n",
      "Episode#:146 ep_reward:166.0\n",
      "\n",
      "Episode#:147 ep_reward:168.0\n",
      "\n",
      "Episode#:148 ep_reward:142.0\n",
      "\n",
      "Episode#:149 ep_reward:137.0\n",
      "\n",
      "Episode#:150 ep_reward:188.0\n",
      "\n",
      "Episode#:151 ep_reward:106.0\n",
      "\n",
      "Episode#:152 ep_reward:136.0\n",
      "\n",
      "Episode#:153 ep_reward:104.0\n",
      "\n",
      "Episode#:154 ep_reward:163.0\n",
      "\n",
      "Episode#:155 ep_reward:199.0\n",
      "\n",
      "Episode#:156 ep_reward:199.0\n",
      "\n",
      "Episode#:157 ep_reward:199.0\n",
      "\n",
      "Episode#:158 ep_reward:199.0\n",
      "\n",
      "Episode#:159 ep_reward:165.0\n",
      "\n",
      "Episode#:160 ep_reward:149.0\n",
      "\n",
      "Episode#:161 ep_reward:199.0\n",
      "\n",
      "Episode#:162 ep_reward:199.0\n",
      "\n",
      "Episode#:163 ep_reward:84.0\n",
      "\n",
      "Episode#:164 ep_reward:199.0\n",
      "\n",
      "Episode#:165 ep_reward:199.0\n",
      "\n",
      "Episode#:166 ep_reward:199.0\n",
      "\n",
      "Episode#:167 ep_reward:199.0\n",
      "\n",
      "Episode#:168 ep_reward:19.0\n",
      "\n",
      "Episode#:169 ep_reward:199.0\n",
      "\n",
      "Episode#:170 ep_reward:199.0\n",
      "\n",
      "Episode#:171 ep_reward:199.0\n",
      "\n",
      "Episode#:172 ep_reward:199.0\n",
      "\n",
      "Episode#:173 ep_reward:199.0\n",
      "\n",
      "Episode#:174 ep_reward:199.0\n",
      "\n",
      "Episode#:175 ep_reward:199.0\n",
      "\n",
      "Episode#:176 ep_reward:126.0\n",
      "\n",
      "Episode#:177 ep_reward:199.0\n",
      "\n",
      "Episode#:178 ep_reward:199.0\n",
      "\n",
      "Episode#:179 ep_reward:199.0\n",
      "\n",
      "Episode#:180 ep_reward:199.0\n",
      "\n",
      "Episode#:181 ep_reward:199.0\n",
      "\n",
      "Episode#:182 ep_reward:199.0\n",
      "\n",
      "Episode#:183 ep_reward:199.0\n",
      "\n",
      "Episode#:184 ep_reward:199.0\n",
      "\n",
      "Episode#:185 ep_reward:199.0\n",
      "\n",
      "Episode#:186 ep_reward:199.0\n",
      "\n",
      "Episode#:187 ep_reward:199.0\n",
      "\n",
      "Episode#:188 ep_reward:36.0\r"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "#env = gym.make('MountainCar-v0')\n",
    "action_dim = env.action_space.n # 2 for the cartpole\n",
    "agent = Agent(action_dim = action_dim)\n",
    "render = True\n",
    "\n",
    "for episode in range(300):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    #done = False\n",
    "    \n",
    "    for step in range(200):\n",
    "    #while not done:\n",
    "        action = agent.act(state)\n",
    "        # Append the state before applying the action\n",
    "        agent.states.append(state)\n",
    "        agent.actions.append(action)\n",
    "        \n",
    "        state, reward, done, _ = env.step(action)\n",
    "        agent.rewards.append(reward)\n",
    "        \n",
    "        total_reward += reward\n",
    "        \n",
    "        \n",
    "        if render:\n",
    "            env.render()\n",
    "        \n",
    "        if done:\n",
    "            agent.learn()\n",
    "            agent.onpolicy_reset()\n",
    "            print(\"\\n\")\n",
    "            break\n",
    "        print(f\"Episode#:{episode} ep_reward:{total_reward}\", end=\"\\r\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
